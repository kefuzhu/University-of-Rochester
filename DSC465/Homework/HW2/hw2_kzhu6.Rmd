---
title: "DSC 465, Homework 2"
author: "Kefu Zhu"
date: "02/08/2019"
output: 
  html_document:
    highlight: tango
    toc: yes
---

```{r}
# Load library
library(survival)
library(MASS)
library(graphics)
library(latex2exp)
```


# Question 1

## (a)

$\because L(p;x) = \prod_{i=1}^n {x_i -1 \choose r-1} p^r (1-p)^{x_i-r}$

Take the log of the likelihood, then we have 

$log(L(p;x)) = \sum_{i=1}^n log{x_i - 1 \choose r - 1} + rlog(p) + (x_i -r) log(1-p)$

<br>
Take the derivative of the log likelihood with respect to $p$ to find $\widehat{p}_{MLE}$

<br>
<center>
$\frac{\partial\ log(L(p;x))}{\partial\ p} = \sum_{i=1}^n \frac{r}{p} - \frac{x_i - p}{1-p} = 0$

$\frac{nr}{p} = \frac{\sum_{i=1}^n x_i - nr}{1-p}$

$\widehat{p} = \frac{nr}{\sum_{i=1}^n x_i}$
</center>

<br>
For a single observation $X$, $\ \widehat{p}_{MLE} = \frac{r}{X}$

## (b)

$\because$

$\pi(p) = \frac{1}{B(\alpha,\beta)} p^{\alpha -1}(1-p)^{\beta-1}$

$P(X=x|p) = {x-1 \choose r-1}p^r(1-p)^{x-r}$

$\therefore$
$\pi(p|x) = \frac{P(X=x|p)\ \pi(p)}{\int_{p=0}^1 P(X=x|p)\ \pi(p) \ \mathrm{d}p} = \frac{{x-1 \choose r-1}p^r(1-p)^{x-r} \cdot \frac{1}{B(\alpha,\beta)} p^{\alpha -1}(1-p)^{\beta-1}}{\int_{p=0}^1 {x-1 \choose r-1}p^r(1-p)^{x-r} \cdot \frac{1}{B(\alpha,\beta)} p^{\alpha -1}(1-p)^{\beta-1}\ \mathrm{d}p} \propto p^{r+\alpha-1}(1-p)^{x-r+\beta-1}$

<br>
Hence, the posterior density of $p$ given observation $X=x$ where $X$ ~ $nb(n,p)$ is $beta(r+\alpha, x-r+\beta)$, which is an example of **conjugate prior**

## (c)

$\because p_{prior}$ ~ $beta(\alpha, \beta)$

$\therefore E[\frac{1}{p_{prior}}] = \int_0^1 \frac{1}{p} \cdot \frac{p^{\alpha -1} (1-p)^{\beta - 1}}{B(\alpha,\beta)} = \frac{B(\alpha - 1, \beta)}{B(\alpha, \beta)} = \frac{\Gamma(\alpha-1)\Gamma(\beta)}{\Gamma(\alpha+\beta-1)} \cdot \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha-1)\Gamma(\beta)} = \frac{\alpha + \beta - 1}{\alpha -1} \rightarrow \widehat{\mu}_{prior} = \frac{r}{\widehat{p}_{prior}} = \frac{r\ (\alpha+\beta-1)}{\alpha-1}$

$\because p_{post}$ ~ $beta(r+\alpha, x-r+\beta)$

$\therefore E[\frac{1}{p_{post}}] = \int_0^1 \frac{1}{p} \cdot \frac{p^{r +\alpha -1} (1-p)^{x-r+\beta - 1}}{B(r+\alpha,x-r+\beta)} = \frac{B(r+\alpha - 1, x-r+\beta)}{B(r+\alpha, x-r+\beta)} = \frac{x+\alpha + \beta - 1}{r+\alpha -1} \rightarrow \widehat{\mu}_{post} = \frac{r}{\widehat{p}_{post}} = \frac{r\ (x+\alpha+\beta-1)}{r+\alpha-1}$

## (d)

$\widehat{\mu}_{post} = \frac{r}{\widehat{p}_{post}} = \frac{r\ (x+\alpha+\beta-1)}{r+\alpha-1} = \frac{r}{r+\alpha-1} x\ +\ \frac{r\ (\alpha + \beta-1)}{r+\alpha-1}$ 

$= \frac{r}{r+\alpha-1} x\ +\ \frac{\alpha-1}{r + \alpha-1} \cdot \frac{r\ (\alpha + \beta-1)}{\alpha-1}$

$= \frac{r}{r+\alpha-1} x\ +\ (1 - \frac{r}{r+\alpha-1}) \cdot \frac{r\ (\alpha + \beta-1)}{\alpha-1}$

<br>
Define $q = \frac{r}{r+\alpha-1}$, then we can write 

<center>
$\widehat{\mu}_{post} = q \cdot \widehat{\mu}_{MLE} + (1-q) \cdot \widehat{\mu}_{prior}$
</center>

# Question 2

## (a)

Define $r(t_i)$ as the number at risk and $d_i$ as the number of deaths

| $t_i$ | $d_i$ | $r(t_i)$ | $\widehat{p}_i$ | $\widehat{S}_t$ |
|:-----:|:---:|:------:|:------:|:-----:|
| 0 | 0 | 8 | 1 | 1 |
| 105 | 1 | 8 | $\frac{7}{8}$ | $\frac{7}{8}$ |
| 107.5 | 2 | 7 | $\frac{5}{7}$ | $\frac{5}{8}$ |
| 110 | 0 | 4 | 1 | $\frac{5}{8}$ |
| 115 | 2 | 3 | $\frac{1}{3}$ | $\frac{5}{24}$ |
| 120 | 1 | 1 | 0 | 0 |

## (b)

### (i)
```{r, fig.align='center', warning=FALSE}
t = c(0,105,105,107.5,107.5,110,110,115,115,120,120)
s = c(1,1,7/8,7/8,5/8,5/8,5/8,5/8,5/24,5/24,0)
plot(t,s,type = 'l')
censor_t = c(107.5,110)
censor_s = c(5/8,5/8)
points(censor_t,censor_s, pch = 3)
```

### (ii)
```{r, fig.align='center'}
x = c(105,107.5,107.5,107.5,110,115,115,120)
not_censor = c(1,1,1,0,0,1,1,1)
plot(survfit(Surv(x,not_censor)~1), conf.int=FALSE,mark.time=TRUE)
```

**Answer**: The two curves are the same.

# Question 3

## (a)

Given a hazard rate $h(x)$, its corresponding cumulative hazard rate is 

<center>
$H(x) = \int_{y=0}^x h(y)\ \mathrm{d}y$
</center>

If exists another hazard rate $h'(x)$ that is proportional to $h(x)$ such as $h'(x) = C \cdot h(x)$, then the cumulative harzard rate for $h'(x)$ can be written as 

<center>
$H'(x) = \int_{y=0}^x C \cdot h'(y)\ \mathrm{d}y = C \cdot \int_{y=0}^x h'(y)\ \mathrm{d}y = C \cdot H(x)$
</center>

Therefore, if the hazard rates are proportional, the cumulative hazard rates will be proportional as well.

## (b)

```{r, fig.align='center'}
KM = survfit(Surv(stime, status) ~ cell,data = VA)
plot(KM, col = c('red', 'blue', 'green', 'violet'), 
     xlab = 'days', ylab = 'Survival', main = 'Kaplan-Meier Estimate of Survival Curves')
legend('topright',
        legend = c("Cell Type 1", "Cell Type 2", 'Cell Type 3', 'Cell Type 4'),
        col = c('red', 'blue', 'green', 'violet'),
        lty = 1,
        cex = 0.5)
```

```{r, fig.align='center'}
# Cumulative hazard functions
plot(survfit(Surv(stime, status) ~ cell, data = VA), fun = "cumhaz",
     col = c('red', 'blue', 'green', 'violet'), 
     xlab = 'days', ylab = 'H(x)', main = 'Cumulative Hazard Functions')
legend('bottomright',
        legend = c("Cell Type 1", "Cell Type 2", 'Cell Type 3', 'Cell Type 4'),
        col = c('red', 'blue', 'green', 'violet'),
        lty = 1,
        cex = 0.5)
```

**Answer**: Based on the plot above, the proportional hazards assumption seem reasonable.

## (c)

### (A)
```{r}
fit.A = coxph(Surv(stime,status) ~ cell, data = VA)
```

### (B)
```{r}
fit.B = coxph(Surv(stime,status) ~ cell + Karn, data = VA)
```

### (C)
```{r}
fit.C = coxph(Surv(stime,status) ~ cell * Karn, data = VA)
```

```{r}
# ANOVA test for model A and model B
anova(fit.A, fit.B)
```

```{r}
# ANOVA test for model B and model C
anova(fit.B, fit.C)
```

**Answer**: Based on the ANOVA test above, we can conclude that at a significance level of 5%

- model (B) is significantly better than model (a)
- model (C) is NOT significantly better than model (B)

## (d)

```{r}
# Generic function to creat required plot
survival_plot = function(model, cell_type, model_name){
  # Compute the 25th percentile and 75th percentile value for Karnofsky scofres (Karn)
  Karn_25 = quantile(VA$Karn, 0.25)
  Karn_75 = quantile(VA$Karn, 0.75)
  # Create prediction dataset
  time.grid = seq(0,1000)
  
  new.x_Karn25 = data.frame(
    stime = time.grid,
    status = rep(1, 1001),
    cell = rep(as.factor(cell_type), 1001),
    Karn = rep(Karn_25, 1001))
  
  new.x_Karn75 = data.frame(
    stime = time.grid,
    status = rep(1, 1001),
    cell = rep(as.factor(cell_type), 1001),
    Karn = rep(Karn_75, 1001))
  
  # Predit using 25th percentile of Karn
  pred.Karn_25 = predict(model, newdata = new.x_Karn25, type = 'expected')
  # Predit using 75th percentile of Karn
  pred.Karn_75 = predict(model, newdata = new.x_Karn75, type = 'expected')
  
  # Create Kaplan-Meier plot
  KM = survfit(Surv(stime, status) ~ cell,data = VA[VA$cell == cell_type,])
  plot(KM, lwd = 1.5,
       xlab = 'days', ylab = 'Survival', 
       main = paste('Survival Plot of Cell Type ', 
                    cell_type , ' (', model_name,')',sep = ''),
       conf.int = FALSE)
  # Add estimated survival curve based on Cox proportional hazard models
  lines(x = time.grid, y = exp(-pred.Karn_25), lwd = 1.5, col = 'blue')
  lines(x = time.grid, y = exp(-pred.Karn_75), lwd = 1.5, col = 'orange')
  # Add legend
  legend(
    'topright',
    legend = c(
    "Kaplan-Meier",
    "Cox proportional hazards model (Karn = 25th percentile)",
    "Cox proportional hazards model (Karn = 75th percentile)"
    ),
    col = c('black', 'blue', 'orange'),
    lty = 1,
    cex = 0.7
    )
}
```

### Model B

#### Cell type = 1
```{r, fig.align='center'}
survival_plot(fit.B, cell_type = 1, model_name = 'Model B')
```

#### Cell type = 2
```{r, fig.align='center'}
survival_plot(fit.B, cell_type = 2, model_name = 'Model B')
```

#### Cell type = 3
```{r, fig.align='center'}
survival_plot(fit.B, cell_type = 3, model_name = 'Model B')
```

#### Cell type = 4
```{r, fig.align='center'}
survival_plot(fit.B, cell_type = 4, model_name = 'Model B')
```

### Model C

#### Cell type = 1
```{r, fig.align='center'}
survival_plot(fit.C, cell_type = 1, model_name = 'Model C')
```

#### Cell type = 2
```{r, fig.align='center'}
survival_plot(fit.C, cell_type = 2, model_name = 'Model C')
```

#### Cell type = 3
```{r, fig.align='center'}
survival_plot(fit.C, cell_type = 3, model_name = 'Model C')
```

#### Cell type = 4
```{r, fig.align='center'}
survival_plot(fit.C, cell_type = 4, model_name = 'Model C')
```

## (e)

**Answer**:

From part (c), we already know that model (C) is NOT significantly better than model (B) based on the `anova` test. 

Based on the plots in part (d), we can also notice that when we fix the Karnofsky scores value (`Karn = Q1` or `Karn = Q3`), the survival curve for  have almost the same trend across different cell types. 

Therefore, it is reasonable for us to conclude that predictors `cell` and `Karn` are NOT interactive, which is consistent with the conclusion from part (c)

# Question 4

## (a)

Given a mobile node $\theta = (\theta_x, \theta_y)$ and $\kappa = 8.25$

For stationary node $1$, $(x_1,y_1) = (-0.50,0.00)$, the transimission distance $\mu_1$ can be computed as

<center>
$\mu_1 = \sqrt{(\theta_x + 0.5)^2 + \theta_y^2}$
</center>
<br>

$\because f(z;\kappa,\mu) = \frac{\kappa}{\mu} (\frac{z}{\mu})^{\kappa - 1}e^{-(z/\mu)^{\kappa}}$

$\therefore f(z_1|\theta) = \frac{8.25}{\sqrt{(\theta_x + 0.5)^2 + \theta_y^2}} (\frac{z}{\sqrt{(\theta_x + 0.5)^2 + \theta_y^2}})^{7.25} e^{-(z/\sqrt{(\theta_x + 0.5)^2 + \theta_y^2})^{8.25}}$

Similarly, for stationary $1$ and $2$, since $(x_2,y_2) = (0.42,2.00), (x_3,y_3) = (1.50,1.27)$, 

we have 

<center>
$\mu_2 = \sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2}$

$\mu_3 = \sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2}$
</center>

Following the same computation, we can derive that

$f(z_2|\theta) = \frac{8.25}{\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2}} (\frac{z}{\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2}})^{7.25} e^{-(z/\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2})^{8.25}}$

$f(z_3|\theta) = \frac{8.25}{\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2}} (\frac{z}{\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2}})^{7.25} e^{-(z/\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2})^{8.25}}$

Since the strength of signal for three stationary nodes are independent from each other, we can write

$f(z_1,z_2,z_3|\theta) = f(z_1|\theta) \cdot f(z_2|\theta) \cdot f(z_3|\theta)$ 

$= \frac{8.25}{\sqrt{(\theta_x + 0.5)^2 + \theta_y^2}} (\frac{z_1}{\sqrt{(\theta_x + 0.5)^2 + \theta_y^2}})^{7.25} e^{-(z_1/\sqrt{(\theta_x + 0.5)^2 + \theta_y^2})^{8.25}} \cdot \frac{8.25}{\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2}} (\frac{z_2}{\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2}})^{7.25} e^{-(z_2/\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2})^{8.25}}$ 

$\cdot \frac{8.25}{\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2}} (\frac{z_3}{\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2}})^{7.25} e^{-(z_3/\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2})^{8.25}}$

Having $\ f(z_1,z_2,z_3|\theta)$, we can now compute the posterior density

$\pi(\theta|z_1,z_2,z_3) \propto f(z_1,z_2,z_3|\theta) \cdot \pi(\theta)$, where $\pi(\theta)$ is a uniform prior density

## (b)
```{r}
# Create the region and grid
xgrid = seq(-0.7,1.0,0.01)
ygrid = seq(-0.1,1.8,0.01)
```

Since now we have a finite region, we can compute the prior density

<br>
<center>
$\pi(\theta) = \frac{1}{(1.0-(-0.7)) \cdot (1.8-(-0.1))} = \frac{1}{1.7 \cdot 1.9}$
</center>

<br>
Then the posterior density $\pi(\theta|z_1,z_2,z_3)$ can be computed as the following

<br>
<center>
$\pi(\theta|z_1,z_2,z_3) \propto f(z_1,z_2,z_3|\theta) \cdot \pi(\theta)$

$= \frac{8.25}{\sqrt{(\theta_x + 0.5)^2 + \theta_y^2}} (\frac{z_1}{\sqrt{(\theta_x + 0.5)^2 + \theta_y^2}})^{7.25} e^{-(z_1/\sqrt{(\theta_x + 0.5)^2 + \theta_y^2})^{8.25}} \cdot$

$\frac{8.25}{\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2}} (\frac{z_2}{\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2}})^{7.25} e^{-(z_2/\sqrt{(\theta_x - 0.42)^2 + (\theta_y-2)^2})^{8.25}} \cdot$ 

$\frac{8.25}{\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2}} (\frac{z_3}{\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2}})^{7.25} e^{-(z_3/\sqrt{(\theta_x - 1.5)^2 + (\theta_y-1.27)^2})^{8.25}} \cdot \frac{1}{1.7 \cdot 1.9}$
</center>

```{r}
# Conditional density function given θx and θy
conditional_density = function(theta_x, theta_y, log = FALSE){
  z1 = 1/0.926
  z2 = 1/0.943
  z3 = 1/0.787
  
  mu_1 = sqrt((theta_x + 0.5) ^ 2 + theta_y ^ 2)
  mu_2 = sqrt((theta_x - 0.42) ^ 2 + (theta_y - 2) ^ 2)
  mu_3 = sqrt((theta_x - 1.5) ^ 2 + (theta_y - 1.27) ^ 2)
  
  f_z1_theta = dweibull(x = z1, shape = 8.25, scale = mu_1)
  f_z2_theta = dweibull(x = z2, shape = 8.25, scale = mu_2)
  f_z3_theta = dweibull(x = z3, shape = 8.25, scale = mu_3)
  
  if(log == TRUE){
    return(log(f_z1_theta) * log(f_z2_theta) * log(f_z3_theta))
  } else{
    return(f_z1_theta * f_z2_theta * f_z3_theta)
  }
    
}
```


```{r}
# Posterior density given the position of mobile node
posterior = function(theta_x, theta_y, log = FALSE) {
  prior_density = 1 / (1.7 * 1.9)
  if(log == TRUE){
    return (log(conditional_density(theta_x, theta_y) * prior_density))
  } else{
    return (conditional_density(theta_x, theta_y) * prior_density)
  }
  
}
```

```{r, fig.align='center'}
persp(
  x = xgrid,
  y = ygrid,
  z = outer(xgrid, ygrid, posterior),
  theta = 0,
  phi = 30,
  xlab = 'x',
  ylab = 'y',
  zlab = 'Posterior Density',
  main = '3D Image of Posterior Density'
  )
```

## (c)

The value of $\theta$ that maximizes $\pi(\theta|z_1,z_2,z_3)$ is $\widehat{\theta}_{MAP} (x)$

<br>
<center>
$\widehat{\theta}_{MAP} (x) = {arg \max}_{\theta}\ f(z_1,z_2,z_3|\theta) \pi(\theta)$
</center>

<br>
However, since our prior is uniform distribution, $\pi(\theta)$ is a constant.

Then ${arg \max}_{\theta}\ f(z_1,z_2,z_3|\theta) \pi(\theta) \propto {arg \max}_{\theta}\ f(z_1,z_2,z_3|\theta)$, which is $\widehat{\theta}_{MLE}$

```{r}
# Index of x and y for the maximum posterior density
which(outer(xgrid, ygrid, posterior) == max(outer(xgrid, ygrid, posterior)), arr.ind =TRUE)
# Values of x and y that gives the maximum posterior density
x_MLE = xgrid[which(outer(xgrid, ygrid, posterior) == max(outer(xgrid, ygrid, posterior)), arr.ind =TRUE)[1]]
y_MLE = ygrid[which(outer(xgrid, ygrid, posterior) == max(outer(xgrid, ygrid, posterior)), arr.ind =TRUE)[2]]
```

```{r}
x_MLE
y_MLE
```

$\therefore \widehat{\theta}_{MLE} \approx$ (`r x_MLE`,`r y_MLE`)

## (d)
```{r}
# Create a vector to store all simulated samples
HM_theta = matrix(nrow = 100000, ncol = 2)
# Initial state
theta = c(0,0)

# Hasting-Metropolis
for(i in 1:100000) {
  theta.old = theta
  theta.new = theta.old + runif(2, -1 / 10, 1 / 10)
  r = exp(posterior(theta.new[1], theta.new[2], log = TRUE) - 
          posterior(theta.old[1], theta.old[2], log = TRUE))
  alpha = min(r,1)
  flip_coin = rbinom(1,1,prob = alpha)
  if(flip_coin == 1){
    HM_theta[i,] = theta.new
    theta = theta.new
  } else{
    HM_theta[i,] = theta.old
    theta = theta.old
  }
}
```

## (e)
```{r, fig.align='center'}
smoothScatter(
  HM_theta,
  xlim = c(-1, 1.5),
  ylim = c(-0.5, 2.1),
  xlab = expression(theta[x]),
  ylab = expression(theta[y]),
  main = 'Hastings-Metropolis Simulation'
  )
contour(xgrid,ygrid,outer(xgrid, ygrid, posterior), add = TRUE)
abline(h=0)
abline(v=0)
points(x = -0.5, y = 0, pch = 17, col = 'red')
points(x = 0.42, y = 2, pch = 17, col = 'red')
points(x = 1.5, y = 1.27, pch = 17, col = 'red')
points(x_MLE,y_MLE, pch = 19, col = 'green')
legend('topright', cex = 0.8,
       legend = c('Stationary Node',TeX('θ_{MLE}')),
       col = c('red','green'),
       pch = c(17,19),
       bg = 'lightblue')
```

## (f)
```{r}
# Function to compute the distance from mobile node to origin
d = function(x){sqrt(x[1]^2 + x[2]^2)}
# Convert the matrix to list
HM_theta_list = split(HM_theta, 1:nrow(HM_theta))
# Compute the distance to origin for each sample mobile node
origin_distance = sapply(HM_theta_list, d)
```

```{r, fig.align='center'}
# Histogram
hist(origin_distance,
     nclass = 100,
     xlab = 'Distance from the Origin',
     main = 'Histogram of Distance of Mobile Node from the Origin')
```

# Question 5

## (a)

$\because E[e^{tX}] = \int e^{tX}p(x) \mathrm{d}x$

$\therefore M_X(t) = E[e^{tX}] = \int e^{tX}p(x) \mathrm{d}x$

Then we have 

$\frac{\partial M_X(t)}{\partial t} = \frac{\partial}{\partial t}\int e^{tX}p(x) \mathrm{d}x = \int \frac{\partial}{\partial t} e^{tX}p(x) \mathrm{d}x = \int Xe^{tX}p(x) \mathrm{d}x$

For the $k$th moment of $X$,

$\frac{\partial^k M_X(t)}{\partial t^k} = \int \frac{\partial}{\partial t^k} e^{tX}p(x) \mathrm{d}x = \int X^ke^{tX}p(x) \mathrm{d}x$

$\rightarrow \frac{\mathrm{d}^k M_X(t)}{\mathrm{d}t^k} |_{t=0} = \int X^kp(x) \mathrm{d}x = E[X^k]$

## (b)

As described in the question, in natural parameteriazation, we have 

$\int_{-\infty}^{+\infty} f(x|\eta)\ \mathrm{d}x = \int_{-\infty}^{+\infty} \exp\{\eta T(x) - A(\eta)\}h(x)\ \mathrm{d}x = 1$ 

$\Rightarrow \int_{-\infty}^{+\infty} e^{\eta T(x)}h(x) = e^{A(\eta)}$

For the moment generating function of $T(x)$,

$m(t) = E[e^{tT(X)}] = \int_{-\infty}^{+\infty} e^{tT(X)} f(x|\eta)\ \mathrm{d}x = \int_{-\infty}^{+\infty} e^{tT(X)} e^{\eta T(x) - A(\eta)} h(x)\ \mathrm{d}x$

$= e^{-A(\eta)} \int_{-\infty}^{+\infty} e^{(t+\eta)T(x)}h(x)\ \mathrm{d}x$

$= e^{-A(\eta)} \cdot e^{A(t+\eta)}$

$= e^{A(t+\eta) - A(\eta)}$

Based on the result from part (a), we know that $\frac{\mathrm{d}^k m(t)}{\mathrm{d}t^k} |_{t=0} = E[T(X)^k]$

Therefore, we can write the mean of $T(X)$, $E[T(X)]$, as the following

<br>
<center>
$E[T(X)] = \frac{\mathrm{d}m(t)}{\mathrm{d}t} |_{t=0} = (e^{A(t+\eta) - A(\eta)} \cdot A'(t+\eta))|_{t=0} = A'(\eta)$
</center>

<br>
Similarly, we have

$E[T(X)^2] = \frac{\mathrm{d^2}m(t)}{\mathrm{d}t^2} |_{t=0} = (e^{A(t+\eta) - A(\eta)} \cdot A''(t+\eta) + e^{A(t+\eta) - A(\eta)} \cdot A'(t+\eta)^2)|_{t=0} = A''(\eta) + A'(\eta)^2$


Then the variance of $T(X)$ can be written as

<center>
$var(T(X)) = E[T(X)^2] - E^2[T(X)] = A''(\eta)$
</center>

## (c)

$\because L(\eta;X) = \ln f(x|\eta) = \eta T(X) - A(\eta) + \ln h(x)$

$\therefore$ 

$\frac{\mathrm{d} L(\eta;X)}{\mathrm{d}\eta} = T(X) - A'(\eta)$

$\frac{\mathrm{d^2} L(\eta;X)}{\mathrm{d}\eta^2} = - A''(\eta) < 0$

Notice that since the second derivative of the log likelihood function of the natural parameter is less than zero, it must be a concave function. To maximize $L(\eta;X)$, we take the derivate with respect to $\eta$ and set it to zero

<center>
$\frac{\mathrm{d} L(\eta;X)}{\mathrm{d}\eta} = T(X) - A'(\eta) = 0$

$T(X) = A'(\eta)$
</center>

<br>
Based on the result from part (b), we know that $E[T(X)] = A'(\eta)$. So we prove that if $\widehat{\eta} = \widehat{\eta}(X)$ is any solution to the equation $T(X) = E_{\eta}[T(X)]$ with respect to $\eta$, then it uniquely maximizes the the log likelihood function $L(\eta;X)$


